{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyanshbajpai/EIP4-Coursework/blob/master/Week-5/Copy_of_PersonAttrubutes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "c4d05cc7-4206-4286-b8a9-5e09017ac779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "bbb906f1-1445-4655-c4cb-f023a6180234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "f3eaadf6-3cdc-418f-edee-d71abc462ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "5223bf02-f3c1-4fec-c6ac-783be105929b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcGkAwmCor0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "942a2fae-d39a-4c3f-acc7-33d04eb7e6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10858, 28), (2715, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "18d70113-6d50-43f2-8a97-b843839904cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6123</th>\n",
              "      <td>resized/6124.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9976</th>\n",
              "      <td>resized/9977.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>resized/782.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10133</th>\n",
              "      <td>resized/10134.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13496</th>\n",
              "      <td>resized/13498.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "6123    resized/6124.jpg              0  ...                        0              0\n",
              "9976    resized/9977.jpg              0  ...                        0              0\n",
              "781      resized/782.jpg              1  ...                        1              0\n",
              "10133  resized/10134.jpg              1  ...                        1              0\n",
              "13496  resized/13498.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=128,augmentation=ImageDataGenerator( \n",
        "                                    # set input mean to 0 over the dataset \n",
        "                                    featurewise_center=False, \n",
        "                                    # set each sample mean to 0 \n",
        "                                    samplewise_center=False, \n",
        "                                    # divide inputs by std of dataset \n",
        "                                    featurewise_std_normalization=False, \n",
        "                                    # divide each input by its std \n",
        "                                    samplewise_std_normalization=False, \n",
        "                                    # apply ZCA whitening \n",
        "                                    zca_whitening=False, \n",
        "                                    # epsilon for ZCA whitening \n",
        "                                    zca_epsilon=1e-06, \n",
        "                                    # randomly rotate images in the range (deg 0 to 180) \n",
        "                                    rotation_range=0, \n",
        "                                    # randomly shift images horizontally \n",
        "                                    width_shift_range=0.1, \n",
        "                                    # randomly shift images vertically \n",
        "                                    height_shift_range=0.1, \n",
        "                                    # set range for random shear \n",
        "                                    shear_range=0.1, \n",
        "                                    # set range for random zoom \n",
        "                                    zoom_range=0, \n",
        "                                    # set range for random channel shifts \n",
        "                                    channel_shift_range=0, \n",
        "                                    # set mode for filling points outside the input boundaries \n",
        "                                    fill_mode='nearest', \n",
        "                                    # value used for fill_mode = \"constant\" \n",
        "                                    cval=0, \n",
        "                                    # randomly flip images \n",
        "                                    horizontal_flip=True, \n",
        "                                    # randomly flip images \n",
        "                                    vertical_flip=False, \n",
        "                                    # set rescaling factor (applied before any other transformation) \n",
        "                                    rescale=None, \n",
        "                                    # set function that will be applied on each input \n",
        "                                    preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False), \n",
        "                                    # image data format, either \"channels_first\" or \"channels_last\" \n",
        "                                    data_format=None))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=128, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "e40d8981-1f8d-4716-a004-32257f748b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdqT2C54LRYd",
        "colab_type": "code",
        "outputId": "00dde031-c95e-4d27-99eb-ef2282365fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape[0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "de2dd82a-3ab0-472b-8760-3508b86efb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# backbone = VGG16(\n",
        "#     weights=\"imagenet\", \n",
        "#     include_top=False, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "\n",
        "from keras.applications import ResNet50V2\n",
        "backbone = ResNet50V2(\n",
        "    weights=None,\n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_13[0][0]           \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_14[0][0]           \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_15[0][0]           \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           post_relu[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_69 (Dense)                (None, 512)          51380736    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 512)          0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_71 (Dense)                (None, 128)          65664       dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 128)          65664       dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 128)          65664       dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_77 (Dense)                (None, 128)          65664       dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_79 (Dense)                (None, 128)          65664       dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_81 (Dense)                (None, 128)          65664       dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_85 (Dense)                (None, 128)          65664       dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_83 (Dense)                (None, 128)          65664       dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_71[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_77[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_79[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_81[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_85[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_83[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 75,474,331\n",
            "Trainable params: 75,428,891\n",
            "Non-trainable params: 45,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugKvw583FcKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "def scheduler(epoch, lr): #Implements cyclical learning rate\n",
        "  step_size = 2200 #One triangle is completed at the 46th epoch and the learning rate remains the base rate for the rest 4 epochs\n",
        "  iterations = epoch * 100\n",
        "  base_lr = .005 #base learning rate\n",
        "  max_lr = .5 #max learning rate\n",
        "  cycle = np.floor(1+iterations/(2*step_size))\n",
        "  x = np.abs(iterations/step_size - 2*cycle + 1)\n",
        "  new_lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
        "  print((lr, new_lr))\n",
        "  if(round(lr, 2) <= base_lr and epoch > 1):\n",
        "    return base_lr \n",
        "  else:\n",
        "    return new_lr\n",
        "\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks=[LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "6dae95c7-d48b-46db-9990-774a093556d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "(0.0010000000474974513, 0.005)\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 49s 588ms/step - loss: 9.2426 - gender_output_loss: 0.8116 - image_quality_output_loss: 1.1444 - age_output_loss: 1.6183 - weight_output_loss: 1.1749 - bag_output_loss: 1.1172 - footwear_output_loss: 1.2160 - pose_output_loss: 1.0704 - emotion_output_loss: 1.0898 - gender_output_acc: 0.5283 - image_quality_output_acc: 0.5320 - age_output_acc: 0.3719 - weight_output_acc: 0.6164 - bag_output_acc: 0.5364 - footwear_output_acc: 0.4235 - pose_output_acc: 0.5960 - emotion_output_acc: 0.6940 - val_loss: 8.4079 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 1.0471 - val_age_output_loss: 1.5403 - val_weight_output_loss: 1.0940 - val_bag_output_loss: 0.9725 - val_footwear_output_loss: 1.0688 - val_pose_output_loss: 0.9580 - val_emotion_output_loss: 1.0385 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.3973 - val_age_output_acc: 0.3121 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.4803 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 2/50\n",
            "(0.004999999888241291, 0.02749999999999998)\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.02749999999999998.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.9614 - gender_output_loss: 0.6869 - image_quality_output_loss: 0.9934 - age_output_loss: 1.4412 - weight_output_loss: 1.0071 - bag_output_loss: 0.9277 - footwear_output_loss: 1.0458 - pose_output_loss: 0.9357 - emotion_output_loss: 0.9237 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5470 - age_output_acc: 0.4005 - weight_output_acc: 0.6369 - bag_output_acc: 0.5643 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7153 - 49s 588ms/step - loss: 9.2426 - gender_output_loss: 0.8116 - image_quality_output_loss: 1.1444 - age_output_loss: 1.6183 - weight_output_loss: 1.1749 - bag_output_loss: 1.1172 - footwear_output_loss: 1.2160 - pose_output_loss: 1.0704 - emotion_output_loss: 1.0898 - gender_output_acc: 0.5283 - image_quality_output_acc: 0.5320 - age_output_acc: 0.3719 - weight_output_acc: 0.6164 - bag_output_acc: 0.5364 - footwear_output_acc: 0.4235 - pose_output_acc: 0.5960 - emotion_output_acc: 0.6940 - val_loss: 8.4079 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 1.0471 - val_age_output_loss: 1.5403 - val_weight_output_loss: 1.0940 - val_bag_output_loss: 0.9725 - val_footwear_output_loss: 1.0688 - val_pose_output_loss: 0.9580 - val_emotion_output_loss: 1.0385 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.3973 - val_age_output_acc: 0.3121 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.4803 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.9596 - gender_output_loss: 0.6868 - image_quality_output_loss: 0.9937 - age_output_loss: 1.4401 - weight_output_loss: 1.0080 - bag_output_loss: 0.9279 - footwear_output_loss: 1.0458 - pose_output_loss: 0.9351 - emotion_output_loss: 0.9222 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5461 - age_output_acc: 0.4009 - weight_output_acc: 0.6358 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7159 - val_loss: 7.9161 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4373 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 1.0478 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9363 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 3/50\n",
            "(0.027499999850988388, 0.049999999999999954)\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.049999999999999954.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8583 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9838 - age_output_loss: 1.4247 - weight_output_loss: 0.9817 - bag_output_loss: 0.9173 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9255 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5617 - image_quality_output_acc: 0.5509 - age_output_acc: 0.4022 - weight_output_acc: 0.6364 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4482 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7152(0.027499999850988388, 0.049999999999999954)\n",
            "84/84 [==============================] - 32s 383ms/step - loss: 7.8587 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9849 - age_output_loss: 1.4256 - weight_output_loss: 0.9815 - bag_output_loss: 0.9169 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9246 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5620 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4013 - weight_output_acc: 0.6366 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7151 - val_loss: 7.9241 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9208 - val_footwear_output_loss: 1.0492 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9374 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 4/50\n",
            "(0.05000000074505806, 0.07249999999999994)\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.07249999999999994.\n",
            "84/84 [==============================] - 32s 383ms/step - loss: 7.8587 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9849 - age_output_loss: 1.4256 - weight_output_loss: 0.9815 - bag_output_loss: 0.9169 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9246 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5620 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4013 - weight_output_acc: 0.6366 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7151 - val_loss: 7.9241 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9208 - val_footwear_output_loss: 1.0492 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9374 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 386ms/step - loss: 7.8674 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9852 - age_output_loss: 1.4272 - weight_output_loss: 0.9834 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9265 - emotion_output_loss: 0.9012 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5492 - age_output_acc: 0.4007 - weight_output_acc: 0.6359 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4431 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7148 - val_loss: 7.9185 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 1.0475 - val_pose_output_loss: 0.9323 - val_emotion_output_loss: 0.9371 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 5/50\n",
            "(0.07249999791383743, 0.09500000000000003)\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.09500000000000003.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8732 - gender_output_loss: 0.6866 - image_quality_output_loss: 0.9858 - age_output_loss: 1.4282 - weight_output_loss: 0.9848 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9279 - emotion_output_loss: 0.9018 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5491 - age_output_acc: 0.3984 - weight_output_acc: 0.6354 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4472 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7148Epoch 5/50\n",
            "(0.07249999791383743, 0.09500000000000003)\n",
            "84/84 [==============================] - 32s 383ms/step - loss: 7.8701 - gender_output_loss: 0.6866 - image_quality_output_loss: 0.9855 - age_output_loss: 1.4271 - weight_output_loss: 0.9844 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9010 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5495 - age_output_acc: 0.3998 - weight_output_acc: 0.6355 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4476 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7152 - val_loss: 7.9261 - val_gender_output_loss: 0.6861 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4430 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9191 - val_footwear_output_loss: 1.0480 - val_pose_output_loss: 0.9327 - val_emotion_output_loss: 0.9365 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 6/50\n",
            "(0.0949999988079071, 0.11750000000000001)\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.11750000000000001.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8737 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9861 - age_output_loss: 1.4298 - weight_output_loss: 0.9834 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6361 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4362 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7155 - val_loss: 7.9429 - val_gender_output_loss: 0.6931 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.4375 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9233 - val_footwear_output_loss: 1.0502 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9448 - val_gender_output_acc: 0.5629 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3720 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 7/50\n",
            "(0.11749999970197678, 0.13999999999999999)\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.13999999999999999.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8751 - gender_output_loss: 0.6880 - image_quality_output_loss: 0.9878 - age_output_loss: 1.4274 - weight_output_loss: 0.9842 - bag_output_loss: 0.9173 - footwear_output_loss: 1.0421 - pose_output_loss: 0.9286 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5541 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4009 - weight_output_acc: 0.6362 - bag_output_acc: 0.5633 - footwear_output_acc: 0.4338 - pose_output_acc: 0.6179 - emotion_output_acc: 0.716184/84 [==============================] - 32s 378ms/step - loss: 7.8737 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9861 - age_output_loss: 1.4298 - weight_output_loss: 0.9834 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6361 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4362 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7155 - val_loss: 7.9429 - val_gender_output_loss: 0.6931 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.4375 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9233 - val_footwear_output_loss: 1.0502 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9448 - val_gender_output_acc: 0.5629 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3720 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8750 - gender_output_loss: 0.6878 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4278 - weight_output_loss: 0.9838 - bag_output_loss: 0.9171 - footwear_output_loss: 1.0415 - pose_output_loss: 0.9287 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5551 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4009 - weight_output_acc: 0.6364 - bag_output_acc: 0.5637 - footwear_output_acc: 0.4339 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7155 - val_loss: 7.9290 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 0.9787 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 1.0473 - val_pose_output_loss: 0.9321 - val_emotion_output_loss: 0.9364 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 8/50\n",
            "(0.14000000059604645, 0.16249999999999998)\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.16249999999999998.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8735 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9852 - age_output_loss: 1.4280 - weight_output_loss: 0.9831 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9034 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4010 - weight_output_acc: 0.6373 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4380 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(0.14000000059604645, 0.16249999999999998)\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8754 - gender_output_loss: 0.6867 - image_quality_output_loss: 0.9852 - age_output_loss: 1.4279 - weight_output_loss: 0.9844 - bag_output_loss: 0.9179 - footwear_output_loss: 1.0417 - pose_output_loss: 0.9273 - emotion_output_loss: 0.9042 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4014 - weight_output_acc: 0.6368 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4369 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7154 - val_loss: 7.9377 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 0.9874 - val_age_output_loss: 1.4384 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.0486 - val_pose_output_loss: 0.9365 - val_emotion_output_loss: 0.9381 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 9/50\n",
            "(0.16249999403953552, 0.18500000000000005)\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.18500000000000005.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8786 - gender_output_loss: 0.6886 - image_quality_output_loss: 0.9885 - age_output_loss: 1.4288 - weight_output_loss: 0.9825 - bag_output_loss: 0.9190 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9278 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5549 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4015 - weight_output_acc: 0.6365 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4344 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7153 - val_loss: 7.9465 - val_gender_output_loss: 0.6951 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.4389 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9210 - val_footwear_output_loss: 1.0507 - val_pose_output_loss: 0.9341 - val_emotion_output_loss: 0.9364 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 10/50\n",
            "(0.1850000023841858, 0.20750000000000005)\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.20750000000000005.\n",
            "84/84 [==============================] - 32s 384ms/step - loss: 7.8758 - gender_output_loss: 0.6870 - image_quality_output_loss: 0.9863 - age_output_loss: 1.4305 - weight_output_loss: 0.9852 - bag_output_loss: 0.9187 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9264 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5600 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4006 - weight_output_acc: 0.6364 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4389 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7152 - val_loss: 7.9324 - val_gender_output_loss: 0.6884 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4410 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.0508 - val_pose_output_loss: 0.9313 - val_emotion_output_loss: 0.9365 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 11/50\n",
            "(0.20749999582767487, 0.23)\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.23.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8757 - gender_output_loss: 0.6898 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4314 - weight_output_loss: 0.9840 - bag_output_loss: 0.9195 - footwear_output_loss: 1.0401 - pose_output_loss: 0.9264 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5482 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4000 - weight_output_acc: 0.6363 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4469 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7154 - val_loss: 7.9486 - val_gender_output_loss: 0.7025 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4440 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0492 - val_pose_output_loss: 0.9347 - val_emotion_output_loss: 0.9362 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 12/50\n",
            "(0.23000000417232513, 0.2525)\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.2525.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8756 - gender_output_loss: 0.6877 - image_quality_output_loss: 0.9845 - age_output_loss: 1.4273 - weight_output_loss: 0.9855 - bag_output_loss: 0.9189 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9036 - gender_output_acc: 0.5593 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4002 - weight_output_acc: 0.6352 - bag_output_acc: 0.5637 - footwear_output_acc: 0.4385 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7146(0.23000000417232513, 0.2525)\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.2525.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8738 - gender_output_loss: 0.6879 - image_quality_output_loss: 0.9841 - age_output_loss: 1.4271 - weight_output_loss: 0.9852 - bag_output_loss: 0.9183 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9024 - gender_output_acc: 0.5587 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4382 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7152 - val_loss: 7.9343 - val_gender_output_loss: 0.6872 - val_image_quality_output_loss: 0.9777 - val_age_output_loss: 1.4392 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.9255 - val_footwear_output_loss: 1.0474 - val_pose_output_loss: 0.9338 - val_emotion_output_loss: 0.9390 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 13/50\n",
            "(0.2524999976158142, 0.27499999999999997)\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.27499999999999997.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8780 - gender_output_loss: 0.6892 - image_quality_output_loss: 0.9864 - age_output_loss: 1.4282 - weight_output_loss: 0.9823 - bag_output_loss: 0.9207 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9290 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5480 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4011 - weight_output_acc: 0.6365 - bag_output_acc: 0.5633 - footwear_output_acc: 0.4411 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7155 - val_loss: 7.9201 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0474 - val_pose_output_loss: 0.9307 - val_emotion_output_loss: 0.9381 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 14/50\n",
            "(0.2750000059604645, 0.2975)\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.2975.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8728 - gender_output_loss: 0.6873 - image_quality_output_loss: 0.9860 - age_output_loss: 1.4280 - weight_output_loss: 0.9837 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0413 - pose_output_loss: 0.9289 - emotion_output_loss: 0.8996 - gender_output_acc: 0.5584 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4015 - weight_output_acc: 0.6365 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4415 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7159 - val_loss: 7.9279 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 1.0473 - val_pose_output_loss: 0.9322 - val_emotion_output_loss: 0.9400 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 15/50\n",
            "(0.29750001430511475, 0.31999999999999995)\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.31999999999999995.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8853 - gender_output_loss: 0.6872 - image_quality_output_loss: 0.9919 - age_output_loss: 1.4290 - weight_output_loss: 0.9830 - bag_output_loss: 0.9187 - footwear_output_loss: 1.0443 - pose_output_loss: 0.9292 - emotion_output_loss: 0.9021 - gender_output_acc: 0.5550 - image_quality_output_acc: 0.5491 - age_output_acc: 0.4016 - weight_output_acc: 0.6359 - bag_output_acc: 0.5633 - footwear_output_acc: 0.4355 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7146 - val_loss: 7.9497 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4393 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9269 - val_footwear_output_loss: 1.0612 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9372 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 16/50\n",
            "(0.3199999928474426, 0.3424999999999999)\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.3424999999999999.\n",
            "84/84 [==============================] - 31s 375ms/step - loss: 7.8806 - gender_output_loss: 0.6890 - image_quality_output_loss: 0.9911 - age_output_loss: 1.4299 - weight_output_loss: 0.9827 - bag_output_loss: 0.9179 - footwear_output_loss: 1.0418 - pose_output_loss: 0.9283 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5539 - image_quality_output_acc: 0.5497 - age_output_acc: 0.3983 - weight_output_acc: 0.6364 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4372 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7154 - val_loss: 7.9465 - val_gender_output_loss: 0.6944 - val_image_quality_output_loss: 0.9895 - val_age_output_loss: 1.4367 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.9205 - val_footwear_output_loss: 1.0549 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 17/50\n",
            "(0.3425000011920929, 0.365)\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.365.\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8858 - gender_output_loss: 0.6901 - image_quality_output_loss: 0.9885 - age_output_loss: 1.4289 - weight_output_loss: 0.9842 - bag_output_loss: 0.9194 - footwear_output_loss: 1.0429 - pose_output_loss: 0.9292 - emotion_output_loss: 0.9026 - gender_output_acc: 0.5475 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4019 - weight_output_acc: 0.6363 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4383 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7148 - val_loss: 7.9351 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9185 - val_footwear_output_loss: 1.0486 - val_pose_output_loss: 0.9323 - val_emotion_output_loss: 0.9378 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 18/50\n",
            "(0.36500000953674316, 0.3875)\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.3875.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8948 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9877 - age_output_loss: 1.4294 - weight_output_loss: 0.9870 - bag_output_loss: 0.9202 - footwear_output_loss: 1.0457 - pose_output_loss: 0.9287 - emotion_output_loss: 0.9040 - gender_output_acc: 0.5472 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4016 - weight_output_acc: 0.6356 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4324 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7143Epoch 19/50\n",
            "(0.38749998807907104, 0.4100000000000001)\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.4100000000000001.\n",
            "84/84 [==============================] - 31s 370ms/step - loss: 7.8883 - gender_output_loss: 0.6948 - image_quality_output_loss: 0.9864 - age_output_loss: 1.4287 - weight_output_loss: 0.9831 - bag_output_loss: 0.9224 - footwear_output_loss: 1.0452 - pose_output_loss: 0.9267 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5418 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4013 - weight_output_acc: 0.6366 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4329 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7156 - val_loss: 7.9607 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9876 - val_age_output_loss: 1.4415 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9337 - val_footwear_output_loss: 1.0504 - val_pose_output_loss: 0.9416 - val_emotion_output_loss: 0.9355 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5618 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 20/50\n",
            "(0.4099999964237213, 0.43250000000000005)\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.43250000000000005.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8925 - gender_output_loss: 0.6916 - image_quality_output_loss: 0.9894 - age_output_loss: 1.4310 - weight_output_loss: 0.9861 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0448 - pose_output_loss: 0.9297 - emotion_output_loss: 0.9017 - gender_output_acc: 0.5472 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4013 - weight_output_acc: 0.6365 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4344 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7158 - val_loss: 7.9507 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9929 - val_bag_output_loss: 0.9218 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9312 - val_emotion_output_loss: 0.9547 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 21/50\n",
            "(0.4325000047683716, 0.455)\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.455.\n",
            "84/84 [==============================] - 31s 372ms/step - loss: 7.8897 - gender_output_loss: 0.6896 - image_quality_output_loss: 0.9865 - age_output_loss: 1.4293 - weight_output_loss: 0.9872 - bag_output_loss: 0.9217 - footwear_output_loss: 1.0437 - pose_output_loss: 0.9289 - emotion_output_loss: 0.9029 - gender_output_acc: 0.5458 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4020 - weight_output_acc: 0.6364 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4334 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7149 - val_loss: 7.9568 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4397 - val_weight_output_loss: 1.0027 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 1.0562 - val_pose_output_loss: 0.9344 - val_emotion_output_loss: 0.9377 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3724 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 22/50\n",
            "(0.45500001311302185, 0.47750000000000004)\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.47750000000000004.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8878 - gender_output_loss: 0.6873 - image_quality_output_loss: 0.9897 - age_output_loss: 1.4317 - weight_output_loss: 0.9864 - bag_output_loss: 0.9217 - footwear_output_loss: 1.0416 - pose_output_loss: 0.9285 - emotion_output_loss: 0.9008 - gender_output_acc: 0.5545 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4013 - weight_output_acc: 0.6367 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4391 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7153 - val_loss: 7.9486 - val_gender_output_loss: 0.6920 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4397 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0575 - val_pose_output_loss: 0.9360 - val_emotion_output_loss: 0.9435 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 23/50\n",
            "(0.47749999165534973, 0.5)\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.5.\n",
            "84/84 [==============================] - 32s 375ms/step - loss: 7.9008 - gender_output_loss: 0.6906 - image_quality_output_loss: 0.9899 - age_output_loss: 1.4331 - weight_output_loss: 0.9860 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0489 - pose_output_loss: 0.9298 - emotion_output_loss: 0.9027 - gender_output_acc: 0.5564 - image_quality_output_acc: 0.5493 - age_output_acc: 0.4004 - weight_output_acc: 0.6362 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4275 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7153 - val_loss: 7.9411 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9230 - val_footwear_output_loss: 1.0594 - val_pose_output_loss: 0.9317 - val_emotion_output_loss: 0.9394 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 24/50\n",
            "(0.5, 0.47750000000000004)\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.47750000000000004.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 8.0576 - gender_output_loss: 0.6941 - image_quality_output_loss: 0.9949 - age_output_loss: 1.4319 - weight_output_loss: 1.0166 - bag_output_loss: 0.9340 - footwear_output_loss: 1.0782 - pose_output_loss: 0.9698 - emotion_output_loss: 0.9381 - gender_output_acc: 0.5426 - image_quality_output_acc: 0.5479 - age_output_acc: 0.4009 - weight_output_acc: 0.6323 - bag_output_acc: 0.5603 - footwear_output_acc: 0.4235 - pose_output_acc: 0.6080 - emotion_output_acc: 0.7117 - val_loss: 7.9448 - val_gender_output_loss: 0.6892 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.4376 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.9243 - val_footwear_output_loss: 1.0479 - val_pose_output_loss: 0.9340 - val_emotion_output_loss: 0.9376 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 25/50\n",
            "(0.47749999165534973, 0.455)\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.455.\n",
            "84/84 [==============================] - 32s 382ms/step - loss: 7.8964 - gender_output_loss: 0.6894 - image_quality_output_loss: 0.9879 - age_output_loss: 1.4320 - weight_output_loss: 0.9905 - bag_output_loss: 0.9210 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9306 - emotion_output_loss: 0.9044 - gender_output_acc: 0.5540 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4010 - weight_output_acc: 0.6364 - bag_output_acc: 0.5646 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7146 - val_loss: 7.9700 - val_gender_output_loss: 0.6972 - val_image_quality_output_loss: 0.9799 - val_age_output_loss: 1.4443 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9193 - val_footwear_output_loss: 1.0532 - val_pose_output_loss: 0.9356 - val_emotion_output_loss: 0.9568 - val_gender_output_acc: 0.4368 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 26/50\n",
            "(0.45500001311302185, 0.43249999999999994)\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.43249999999999994.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8966 - gender_output_loss: 0.6917 - image_quality_output_loss: 0.9898 - age_output_loss: 1.4350 - weight_output_loss: 0.9854 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9047 - gender_output_acc: 0.5445 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3894 - weight_output_acc: 0.6357 - bag_output_acc: 0.5635 - footwear_output_acc: 0.4378 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7149 - val_loss: 7.9354 - val_gender_output_loss: 0.6959 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 1.0519 - val_pose_output_loss: 0.9330 - val_emotion_output_loss: 0.9357 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 27/50\n",
            "(0.4325000047683716, 0.41)\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.41.\n",
            "84/84 [==============================] - 31s 375ms/step - loss: 7.9025 - gender_output_loss: 0.6902 - image_quality_output_loss: 0.9929 - age_output_loss: 1.4300 - weight_output_loss: 0.9869 - bag_output_loss: 0.9252 - footwear_output_loss: 1.0453 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9044 - gender_output_acc: 0.5500 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4006 - weight_output_acc: 0.6360 - bag_output_acc: 0.5558 - footwear_output_acc: 0.4366 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7152 - val_loss: 7.9685 - val_gender_output_loss: 0.6865 - val_image_quality_output_loss: 0.9912 - val_age_output_loss: 1.4389 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9255 - val_footwear_output_loss: 1.0702 - val_pose_output_loss: 0.9340 - val_emotion_output_loss: 0.9385 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 28/50\n",
            "(0.4099999964237213, 0.3875)\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.3875.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8877 - gender_output_loss: 0.6873 - image_quality_output_loss: 0.9861 - age_output_loss: 1.4298 - weight_output_loss: 0.9851 - bag_output_loss: 0.9197 - footwear_output_loss: 1.0459 - pose_output_loss: 0.9317 - emotion_output_loss: 0.9021 - gender_output_acc: 0.5555 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4000 - weight_output_acc: 0.6357 - bag_output_acc: 0.5631 - footwear_output_acc: 0.4305 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7150Epoch 28/50\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8881 - gender_output_loss: 0.6878 - image_quality_output_loss: 0.9863 - age_output_loss: 1.4298 - weight_output_loss: 0.9851 - bag_output_loss: 0.9197 - footwear_output_loss: 1.0457 - pose_output_loss: 0.9312 - emotion_output_loss: 0.9025 - gender_output_acc: 0.5548 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4004 - weight_output_acc: 0.6359 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4308 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7148 - val_loss: 7.9586 - val_gender_output_loss: 0.6931 - val_image_quality_output_loss: 0.9875 - val_age_output_loss: 1.4444 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0548 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.9379 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 29/50\n",
            "(0.38749998807907104, 0.365)\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.365.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8881 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9870 - age_output_loss: 1.4324 - weight_output_loss: 0.9872 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9044 - gender_output_acc: 0.5547 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4009 - weight_output_acc: 0.6357 - bag_output_acc: 0.5646 - footwear_output_acc: 0.4309 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7151Epoch 29/50\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8859 - gender_output_loss: 0.6888 - image_quality_output_loss: 0.9866 - age_output_loss: 1.4317 - weight_output_loss: 0.9867 - bag_output_loss: 0.9194 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9031 - gender_output_acc: 0.5547 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4008 - weight_output_acc: 0.6362 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4314 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7156 - val_loss: 7.9520 - val_gender_output_loss: 0.6893 - val_image_quality_output_loss: 0.9844 - val_age_output_loss: 1.4460 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0532 - val_pose_output_loss: 0.9336 - val_emotion_output_loss: 0.9436 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 30/50\n",
            "(0.36500000953674316, 0.3425)\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.3425.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8795 - gender_output_loss: 0.6885 - image_quality_output_loss: 0.9858 - age_output_loss: 1.4300 - weight_output_loss: 0.9842 - bag_output_loss: 0.9195 - footwear_output_loss: 1.0433 - pose_output_loss: 0.9255 - emotion_output_loss: 0.9028 - gender_output_acc: 0.5558 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4011 - weight_output_acc: 0.6367 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4328 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7155 - val_loss: 7.9301 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4401 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.0511 - val_pose_output_loss: 0.9379 - val_emotion_output_loss: 0.9368 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 31/50\n",
            "(0.3425000011920929, 0.32000000000000006)\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.32000000000000006.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8826 - gender_output_loss: 0.6882 - image_quality_output_loss: 0.9869 - age_output_loss: 1.4323 - weight_output_loss: 0.9854 - bag_output_loss: 0.9189 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9268 - emotion_output_loss: 0.9013 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5499 - age_output_acc: 0.3956 - weight_output_acc: 0.6365 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4369 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7156 - val_loss: 7.9358 - val_gender_output_loss: 0.6878 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.4399 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9230 - val_footwear_output_loss: 1.0497 - val_pose_output_loss: 0.9351 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 32/50\n",
            "(0.3199999928474426, 0.2975)\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.2975.\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8789 - gender_output_loss: 0.6877 - image_quality_output_loss: 0.9890 - age_output_loss: 1.4281 - weight_output_loss: 0.9834 - bag_output_loss: 0.9219 - footwear_output_loss: 1.0412 - pose_output_loss: 0.9267 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5584 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4018 - weight_output_acc: 0.6366 - bag_output_acc: 0.5632 - footwear_output_acc: 0.4385 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7154 - val_loss: 7.9401 - val_gender_output_loss: 0.6912 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4409 - val_weight_output_loss: 0.9948 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.0480 - val_pose_output_loss: 0.9340 - val_emotion_output_loss: 0.9356 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 33/50\n",
            "(0.29750001430511475, 0.27499999999999997)\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.27499999999999997.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8788 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9866 - age_output_loss: 1.4292 - weight_output_loss: 0.9846 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0446 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9022 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5501 - age_output_acc: 0.3997 - weight_output_acc: 0.6364 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4306 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7150 - val_loss: 7.9339 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9791 - val_age_output_loss: 1.4403 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.9194 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9380 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 34/50\n",
            "(0.2750000059604645, 0.2525)\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.2525.\n",
            "84/84 [==============================] - 31s 369ms/step - loss: 7.8745 - gender_output_loss: 0.6873 - image_quality_output_loss: 0.9849 - age_output_loss: 1.4274 - weight_output_loss: 0.9859 - bag_output_loss: 0.9187 - footwear_output_loss: 1.0414 - pose_output_loss: 0.9266 - emotion_output_loss: 0.9022 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4016 - weight_output_acc: 0.6353 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4403 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7151 - val_loss: 7.9301 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4447 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.9206 - val_footwear_output_loss: 1.0466 - val_pose_output_loss: 0.9327 - val_emotion_output_loss: 0.9350 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 35/50\n",
            "(0.2524999976158142, 0.23)\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.23.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8678 - gender_output_loss: 0.6866 - image_quality_output_loss: 0.9851 - age_output_loss: 1.4278 - weight_output_loss: 0.9834 - bag_output_loss: 0.9174 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9257 - emotion_output_loss: 0.9012 - gender_output_acc: 0.5618 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4019 - weight_output_acc: 0.6364 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7146(0.2524999976158142, 0.23)\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.23.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8662 - gender_output_loss: 0.6866 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4283 - weight_output_loss: 0.9825 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8997 - gender_output_acc: 0.5618 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4013 - weight_output_acc: 0.6367 - bag_output_acc: 0.5635 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7154 - val_loss: 7.9281 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.4405 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0487 - val_pose_output_loss: 0.9338 - val_emotion_output_loss: 0.9351 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 36/50\n",
            "(0.23000000417232513, 0.20750000000000005)\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.20750000000000005.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8680 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9847 - age_output_loss: 1.4271 - weight_output_loss: 0.9847 - bag_output_loss: 0.9167 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5636 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4009 - weight_output_acc: 0.6354 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7155\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8689 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9859 - age_output_loss: 1.4269 - weight_output_loss: 0.9854 - bag_output_loss: 0.9171 - footwear_output_loss: 1.0403 - pose_output_loss: 0.9268 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4013 - weight_output_acc: 0.6355 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7154 - val_loss: 7.9376 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4435 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0489 - val_pose_output_loss: 0.9367 - val_emotion_output_loss: 0.9435 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 37/50\n",
            "(0.20749999582767487, 0.18499999999999994)\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.18499999999999994.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8721 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9842 - age_output_loss: 1.4288 - weight_output_loss: 0.9841 - bag_output_loss: 0.9193 - footwear_output_loss: 1.0413 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9019 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4009 - weight_output_acc: 0.6356 - bag_output_acc: 0.5619 - footwear_output_acc: 0.4437 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7149(0.20749999582767487, 0.18499999999999994)\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8689 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9859 - age_output_loss: 1.4269 - weight_output_loss: 0.9854 - bag_output_loss: 0.9171 - footwear_output_loss: 1.0403 - pose_output_loss: 0.9268 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4013 - weight_output_acc: 0.6355 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7154 - val_loss: 7.9376 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4435 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0489 - val_pose_output_loss: 0.9367 - val_emotion_output_loss: 0.9435 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8716 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9847 - age_output_loss: 1.4286 - weight_output_loss: 0.9838 - bag_output_loss: 0.9193 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9020 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4010 - weight_output_acc: 0.6359 - bag_output_acc: 0.5625 - footwear_output_acc: 0.4439 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7148 - val_loss: 7.9260 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9853 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0508 - val_pose_output_loss: 0.9344 - val_emotion_output_loss: 0.9365 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 38/50\n",
            "(0.1850000023841858, 0.16249999999999998)\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.16249999999999998.\n",
            "84/84 [==============================] - 31s 369ms/step - loss: 7.8711 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9861 - age_output_loss: 1.4284 - weight_output_loss: 0.9843 - bag_output_loss: 0.9194 - footwear_output_loss: 1.0423 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8996 - gender_output_acc: 0.5634 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4002 - weight_output_acc: 0.6355 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4363 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7156 - val_loss: 7.9260 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4401 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 1.0481 - val_pose_output_loss: 0.9329 - val_emotion_output_loss: 0.9354 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 39/50\n",
            "(0.16249999403953552, 0.13999999999999999)\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.13999999999999999.\n",
            "84/84 [==============================] - 31s 370ms/step - loss: 7.8646 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9839 - age_output_loss: 1.4265 - weight_output_loss: 0.9834 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0401 - pose_output_loss: 0.9255 - emotion_output_loss: 0.9008 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4010 - weight_output_acc: 0.6359 - bag_output_acc: 0.5634 - footwear_output_acc: 0.4471 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7145 - val_loss: 7.9278 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4387 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9233 - val_footwear_output_loss: 1.0496 - val_pose_output_loss: 0.9345 - val_emotion_output_loss: 0.9352 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 40/50\n",
            "(0.14000000059604645, 0.11750000000000001)\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.11750000000000001.\n",
            "84/84 [==============================] - 31s 366ms/step - loss: 7.8652 - gender_output_loss: 0.6877 - image_quality_output_loss: 0.9845 - age_output_loss: 1.4264 - weight_output_loss: 0.9839 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0399 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8989 - gender_output_acc: 0.5578 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4011 - weight_output_acc: 0.6358 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4433 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7157 - val_loss: 7.9276 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4394 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0475 - val_pose_output_loss: 0.9350 - val_emotion_output_loss: 0.9367 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 41/50\n",
            "(0.11749999970197678, 0.09500000000000003)\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.09500000000000003.\n",
            "84/84 [==============================] - 31s 367ms/step - loss: 7.8634 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9856 - age_output_loss: 1.4266 - weight_output_loss: 0.9834 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0398 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8988 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4002 - weight_output_acc: 0.6359 - bag_output_acc: 0.5643 - footwear_output_acc: 0.4479 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7155 - val_loss: 7.9243 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.4377 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9194 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9365 - val_emotion_output_loss: 0.9386 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 42/50\n",
            "(0.0949999988079071, 0.07250000000000005)\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.07250000000000005.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8628 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4265 - weight_output_loss: 0.9826 - bag_output_loss: 0.9172 - footwear_output_loss: 1.0399 - pose_output_loss: 0.9274 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4013 - weight_output_acc: 0.6365 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4479 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7152 - val_loss: 7.9161 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.4366 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 1.0487 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9349 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 43/50\n",
            "(0.07249999791383743, 0.049999999999999954)\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.049999999999999954.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8575 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4254 - weight_output_loss: 0.9819 - bag_output_loss: 0.9166 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9246 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5494 - age_output_acc: 0.4010 - weight_output_acc: 0.6362 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4473 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7150 - val_loss: 7.9211 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 0.9777 - val_age_output_loss: 1.4373 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9183 - val_footwear_output_loss: 1.0474 - val_pose_output_loss: 0.9339 - val_emotion_output_loss: 0.9370 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 44/50\n",
            "(0.05000000074505806, 0.02749999999999998)\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.02749999999999998.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8573 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9842 - age_output_loss: 1.4260 - weight_output_loss: 0.9814 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9251 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4009 - weight_output_acc: 0.6366 - bag_output_acc: 0.5643 - footwear_output_acc: 0.4476 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7148 - val_loss: 7.9143 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4359 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9185 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9354 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 45/50\n",
            "(0.027499999850988388, 0.005)\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8553 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9842 - age_output_loss: 1.4248 - weight_output_loss: 0.9823 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8983 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4011 - weight_output_acc: 0.6357 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4470 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7153 - val_loss: 7.9141 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 46/50\n",
            "(0.004999999888241291, 0.02749999999999998)\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 31s 375ms/step - loss: 7.8498 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9834 - age_output_loss: 1.4239 - weight_output_loss: 0.9810 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9251 - emotion_output_loss: 0.8961 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4021 - weight_output_acc: 0.6369 - bag_output_acc: 0.5632 - footwear_output_acc: 0.4469 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 7.9134 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4360 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9357 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 47/50\n",
            "(0.004999999888241291, 0.049999999999999954)\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8548 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4249 - weight_output_loss: 0.9824 - bag_output_loss: 0.9165 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8989 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4009 - weight_output_acc: 0.6359 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4466 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7151 - val_loss: 7.9134 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 48/50\n",
            "(0.004999999888241291, 0.07249999999999994)\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 31s 372ms/step - loss: 7.8524 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9821 - age_output_loss: 1.4249 - weight_output_loss: 0.9817 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9258 - emotion_output_loss: 0.8980 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4010 - weight_output_acc: 0.6364 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4467 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7155 - val_loss: 7.9133 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9324 - val_emotion_output_loss: 0.9357 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 49/50\n",
            "(0.004999999888241291, 0.09499999999999992)\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 31s 374ms/step - loss: 7.8508 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9840 - age_output_loss: 1.4243 - weight_output_loss: 0.9809 - bag_output_loss: 0.9152 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9251 - emotion_output_loss: 0.8974 - gender_output_acc: 0.5620 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4018 - weight_output_acc: 0.6368 - bag_output_acc: 0.5646 - footwear_output_acc: 0.4468 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7159 - val_loss: 7.9136 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 50/50\n",
            "(0.004999999888241291, 0.11749999999999991)\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8531 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9831 - age_output_loss: 1.4251 - weight_output_loss: 0.9815 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9247 - emotion_output_loss: 0.8988 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4005 - weight_output_acc: 0.6358 - bag_output_acc: 0.5635 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7152 - val_loss: 7.9135 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4360 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5636 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4356 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3758bcb400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
