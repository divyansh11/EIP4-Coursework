{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyanshbajpai/EIP4-Coursework/blob/master/Week-5/Assignment-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "c4d05cc7-4206-4286-b8a9-5e09017ac779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "bbb906f1-1445-4655-c4cb-f023a6180234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "f3eaadf6-3cdc-418f-edee-d71abc462ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "5223bf02-f3c1-4fec-c6ac-783be105929b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcGkAwmCor0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "942a2fae-d39a-4c3f-acc7-33d04eb7e6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10858, 28), (2715, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "18d70113-6d50-43f2-8a97-b843839904cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6123</th>\n",
              "      <td>resized/6124.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9976</th>\n",
              "      <td>resized/9977.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>resized/782.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10133</th>\n",
              "      <td>resized/10134.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13496</th>\n",
              "      <td>resized/13498.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "6123    resized/6124.jpg              0  ...                        0              0\n",
              "9976    resized/9977.jpg              0  ...                        0              0\n",
              "781      resized/782.jpg              1  ...                        1              0\n",
              "10133  resized/10134.jpg              1  ...                        1              0\n",
              "13496  resized/13498.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=128,augmentation=ImageDataGenerator( \n",
        "                                    # set input mean to 0 over the dataset \n",
        "                                    featurewise_center=False, \n",
        "                                    # set each sample mean to 0 \n",
        "                                    samplewise_center=False, \n",
        "                                    # divide inputs by std of dataset \n",
        "                                    featurewise_std_normalization=False, \n",
        "                                    # divide each input by its std \n",
        "                                    samplewise_std_normalization=False, \n",
        "                                    # apply ZCA whitening \n",
        "                                    zca_whitening=False, \n",
        "                                    # epsilon for ZCA whitening \n",
        "                                    zca_epsilon=1e-06, \n",
        "                                    # randomly rotate images in the range (deg 0 to 180) \n",
        "                                    rotation_range=0, \n",
        "                                    # randomly shift images horizontally \n",
        "                                    width_shift_range=0.1, \n",
        "                                    # randomly shift images vertically \n",
        "                                    height_shift_range=0.1, \n",
        "                                    # set range for random shear \n",
        "                                    shear_range=0.1, \n",
        "                                    # set range for random zoom \n",
        "                                    zoom_range=0, \n",
        "                                    # set range for random channel shifts \n",
        "                                    channel_shift_range=0, \n",
        "                                    # set mode for filling points outside the input boundaries \n",
        "                                    fill_mode='nearest', \n",
        "                                    # value used for fill_mode = \"constant\" \n",
        "                                    cval=0, \n",
        "                                    # randomly flip images \n",
        "                                    horizontal_flip=True, \n",
        "                                    # randomly flip images \n",
        "                                    vertical_flip=False, \n",
        "                                    # set rescaling factor (applied before any other transformation) \n",
        "                                    rescale=None, \n",
        "                                    # set function that will be applied on each input \n",
        "                                    preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False), \n",
        "                                    # image data format, either \"channels_first\" or \"channels_last\" \n",
        "                                    data_format=None))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=128, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "e40d8981-1f8d-4716-a004-32257f748b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdqT2C54LRYd",
        "colab_type": "code",
        "outputId": "00dde031-c95e-4d27-99eb-ef2282365fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape[0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "d5836726-1eb7-4d71-c75d-999bfaf2abf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# backbone = VGG16(\n",
        "#     weights=\"imagenet\", \n",
        "#     include_top=False, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "\n",
        "from keras.applications import ResNet50V2\n",
        "backbone = ResNet50V2(\n",
        "    weights=None,\n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_10[0][0]           \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_11[0][0]           \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_12[0][0]           \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           post_relu[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 512)          51380736    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 128)          65664       dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_56 (Dense)                (None, 128)          65664       dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (None, 128)          65664       dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (None, 128)          65664       dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_62 (Dense)                (None, 128)          65664       dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_64 (Dense)                (None, 128)          65664       dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_68 (Dense)                (None, 128)          65664       dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_66 (Dense)                (None, 128)          65664       dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_54[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_56[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_60[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_66[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 75,474,331\n",
            "Trainable params: 75,428,891\n",
            "Non-trainable params: 45,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugKvw583FcKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "def scheduler(epoch, lr): #Implements cyclical learning rate\n",
        "  step_size = 2000 #One triangle is completed at the 46th epoch and the learning rate remains the base rate for the rest 4 epochs\n",
        "  iterations = epoch * 100\n",
        "  base_lr = .005 #base learning rate\n",
        "  max_lr = .5 #max learning rate\n",
        "  cycle = np.floor(1+iterations/(2*step_size))\n",
        "  x = np.abs(iterations/step_size - 2*cycle + 1)\n",
        "  new_lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
        "  print((lr, new_lr))\n",
        "  if(round(lr, 2) <= base_lr and epoch > 1):\n",
        "    return base_lr \n",
        "  else:\n",
        "    return new_lr\n",
        "\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks=[LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "5a23154b-f255-4916-a3b9-373a2a031f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 47s 562ms/step - loss: 9.7519 - gender_output_loss: 0.8565 - image_quality_output_loss: 1.2904 - age_output_loss: 1.7106 - weight_output_loss: 1.1847 - bag_output_loss: 1.0896 - footwear_output_loss: 1.3146 - pose_output_loss: 1.1707 - emotion_output_loss: 1.1347 - gender_output_acc: 0.5477 - image_quality_output_acc: 0.5294 - age_output_acc: 0.3850 - weight_output_acc: 0.6177 - bag_output_acc: 0.5419 - footwear_output_acc: 0.4248 - pose_output_acc: 0.6003 - emotion_output_acc: 0.6908 - val_loss: 8.2246 - val_gender_output_loss: 0.6879 - val_image_quality_output_loss: 1.0233 - val_age_output_loss: 1.4869 - val_weight_output_loss: 1.0567 - val_bag_output_loss: 0.9591 - val_footwear_output_loss: 1.0543 - val_pose_output_loss: 0.9644 - val_emotion_output_loss: 0.9920 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 2/50\n",
            "(0.004999999888241291, 0.029750000000000023)\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.029750000000000023.\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8923 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9885 - age_output_loss: 1.4344 - weight_output_loss: 0.9880 - bag_output_loss: 0.9204 - footwear_output_loss: 1.0416 - pose_output_loss: 0.9290 - emotion_output_loss: 0.9048 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4003 - weight_output_acc: 0.6368 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4464 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 7.9138 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9355 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 3/50\n",
            "(0.029750000685453415, 0.05450000000000004)\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.05450000000000004.\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8923 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9885 - age_output_loss: 1.4344 - weight_output_loss: 0.9880 - bag_output_loss: 0.9204 - footwear_output_loss: 1.0416 - pose_output_loss: 0.9290 - emotion_output_loss: 0.9048 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4003 - weight_output_acc: 0.6368 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4464 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 7.9138 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9355 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "\n",
            "84/84 [==============================] - 32s 382ms/step - loss: 7.8626 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9841 - age_output_loss: 1.4276 - weight_output_loss: 0.9821 - bag_output_loss: 0.9178 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9258 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4007 - weight_output_acc: 0.6363 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4472 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7146 - val_loss: 7.9247 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.4371 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.9192 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9320 - val_emotion_output_loss: 0.9401 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 4/50\n",
            "(0.054499998688697815, 0.07924999999999996)\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.07924999999999996.\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8704 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9852 - age_output_loss: 1.4271 - weight_output_loss: 0.9844 - bag_output_loss: 0.9197 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5619 - image_quality_output_acc: 0.5492 - age_output_acc: 0.4012 - weight_output_acc: 0.6357 - bag_output_acc: 0.5632 - footwear_output_acc: 0.4471 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7156 - val_loss: 7.9261 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4436 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.9188 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 0.9340 - val_emotion_output_loss: 0.9360 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 5/50\n",
            "(0.07925000041723251, 0.10399999999999998)\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.10399999999999998.\n",
            "84/84 [==============================] - 32s 386ms/step - loss: 7.8709 - gender_output_loss: 0.6875 - image_quality_output_loss: 0.9848 - age_output_loss: 1.4290 - weight_output_loss: 0.9850 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0396 - pose_output_loss: 0.9278 - emotion_output_loss: 0.8985 - gender_output_acc: 0.5617 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4005 - weight_output_acc: 0.6358 - bag_output_acc: 0.5633 - footwear_output_acc: 0.4477 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7159 - val_loss: 7.9252 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4414 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 1.0476 - val_pose_output_loss: 0.9337 - val_emotion_output_loss: 0.9374 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 6/50\n",
            "(0.10400000214576721, 0.12875)\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.12875.\n",
            "84/84 [==============================] - 32s 383ms/step - loss: 7.8750 - gender_output_loss: 0.6876 - image_quality_output_loss: 0.9850 - age_output_loss: 1.4289 - weight_output_loss: 0.9857 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9013 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4004 - weight_output_acc: 0.6353 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4401 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7149 - val_loss: 7.9241 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.4368 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9232 - val_footwear_output_loss: 1.0475 - val_pose_output_loss: 0.9311 - val_emotion_output_loss: 0.9378 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 7/50\n",
            "(0.1287499964237213, 0.15350000000000003)\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.15350000000000003.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8733 - gender_output_loss: 0.6868 - image_quality_output_loss: 0.9847 - age_output_loss: 1.4292 - weight_output_loss: 0.9854 - bag_output_loss: 0.9198 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9258 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4009 - weight_output_acc: 0.6358 - bag_output_acc: 0.5635 - footwear_output_acc: 0.4425 - pose_output_acc: 0.6193 - emotion_output_acc: 0.715584/84 [==============================] - 32s 383ms/step - loss: 7.8750 - gender_output_loss: 0.6876 - image_quality_output_loss: 0.9850 - age_output_loss: 1.4289 - weight_output_loss: 0.9857 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9013 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4004 - weight_output_acc: 0.6353 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4401 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7149 - val_loss: 7.9241 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.4368 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9232 - val_footwear_output_loss: 1.0475 - val_pose_output_loss: 0.9311 - val_emotion_output_loss: 0.9378 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8721 - gender_output_loss: 0.6867 - image_quality_output_loss: 0.9848 - age_output_loss: 1.4286 - weight_output_loss: 0.9846 - bag_output_loss: 0.9191 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4013 - weight_output_acc: 0.6361 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4424 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7153 - val_loss: 7.9251 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9196 - val_footwear_output_loss: 1.0493 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9380 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 8/50\n",
            "(0.1535000056028366, 0.17825000000000005)\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.17825000000000005.\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8787 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9867 - age_output_loss: 1.4277 - weight_output_loss: 0.9861 - bag_output_loss: 0.9192 - footwear_output_loss: 1.0414 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9034 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4007 - weight_output_acc: 0.6361 - bag_output_acc: 0.5631 - footwear_output_acc: 0.4382 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7148 - val_loss: 7.9535 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4394 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 1.0564 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9437 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 9/50\n",
            "(0.1782499998807907, 0.20299999999999996)\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.20299999999999996.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8817 - gender_output_loss: 0.6875 - image_quality_output_loss: 0.9866 - age_output_loss: 1.4295 - weight_output_loss: 0.9906 - bag_output_loss: 0.9192 - footwear_output_loss: 1.0398 - pose_output_loss: 0.9278 - emotion_output_loss: 0.9008 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4004 - weight_output_acc: 0.6361 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7155 - val_loss: 7.9319 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4373 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.9190 - val_footwear_output_loss: 1.0477 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.9385 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 10/50\n",
            "(0.2029999941587448, 0.22774999999999998)\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.22774999999999998.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8739 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9878 - age_output_loss: 1.4275 - weight_output_loss: 0.9844 - bag_output_loss: 0.9195 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9017 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4013 - weight_output_acc: 0.6360 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4473 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7154 - val_loss: 7.9283 - val_gender_output_loss: 0.6878 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4379 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.9213 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9319 - val_emotion_output_loss: 0.9387 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 11/50\n",
            "(0.2277500033378601, 0.2525)\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.2525.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8774 - gender_output_loss: 0.6880 - image_quality_output_loss: 0.9860 - age_output_loss: 1.4276 - weight_output_loss: 0.9844 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0420 - pose_output_loss: 0.9307 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5555 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4012 - weight_output_acc: 0.6363 - bag_output_acc: 0.5635 - footwear_output_acc: 0.4387 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7156 - val_loss: 7.9209 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4390 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9200 - val_footwear_output_loss: 1.0487 - val_pose_output_loss: 0.9317 - val_emotion_output_loss: 0.9364 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 12/50\n",
            "(0.2524999976158142, 0.27725)\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.27725.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8898 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9878 - age_output_loss: 1.4268 - weight_output_loss: 0.9860 - bag_output_loss: 0.9211 - footwear_output_loss: 1.0444 - pose_output_loss: 0.9283 - emotion_output_loss: 0.9031 - gender_output_acc: 0.5392 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4020 - weight_output_acc: 0.6352 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4278 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7145 - val_loss: 7.9304 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9787 - val_age_output_loss: 1.4384 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9223 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9331 - val_emotion_output_loss: 0.9415 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 13/50\n",
            "(0.2772499918937683, 0.30200000000000005)\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.30200000000000005.\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8821 - gender_output_loss: 0.6889 - image_quality_output_loss: 0.9872 - age_output_loss: 1.4293 - weight_output_loss: 0.9844 - bag_output_loss: 0.9173 - footwear_output_loss: 1.0450 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9020 - gender_output_acc: 0.5507 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4009 - weight_output_acc: 0.6365 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4409 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7148 - val_loss: 7.9271 - val_gender_output_loss: 0.6874 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4373 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0496 - val_pose_output_loss: 0.9316 - val_emotion_output_loss: 0.9361 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 14/50\n",
            "(0.3019999861717224, 0.32675)\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.32675.\n",
            "84/84 [==============================] - 31s 370ms/step - loss: 7.8859 - gender_output_loss: 0.6874 - image_quality_output_loss: 0.9874 - age_output_loss: 1.4301 - weight_output_loss: 0.9881 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0435 - pose_output_loss: 0.9279 - emotion_output_loss: 0.9035 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4021 - weight_output_acc: 0.6363 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4357 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7148 - val_loss: 7.9635 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4484 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.9200 - val_footwear_output_loss: 1.0499 - val_pose_output_loss: 0.9343 - val_emotion_output_loss: 0.9564 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 15/50\n",
            "(0.3267500102519989, 0.3515)\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.3515.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8860 - gender_output_loss: 0.6892 - image_quality_output_loss: 0.9913 - age_output_loss: 1.4297 - weight_output_loss: 0.9858 - bag_output_loss: 0.9178 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9018 - gender_output_acc: 0.5574 - image_quality_output_acc: 0.5489 - age_output_acc: 0.3961 - weight_output_acc: 0.6362 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4342 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7153Epoch 15/50\n",
            "(0.3267500102519989, 0.3515)\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8861 - gender_output_loss: 0.6890 - image_quality_output_loss: 0.9905 - age_output_loss: 1.4297 - weight_output_loss: 0.9853 - bag_output_loss: 0.9172 - footwear_output_loss: 1.0422 - pose_output_loss: 0.9293 - emotion_output_loss: 0.9029 - gender_output_acc: 0.5580 - image_quality_output_acc: 0.5499 - age_output_acc: 0.3960 - weight_output_acc: 0.6363 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4349 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7148 - val_loss: 7.9562 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4444 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9243 - val_footwear_output_loss: 1.0564 - val_pose_output_loss: 0.9366 - val_emotion_output_loss: 0.9488 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 31s 367ms/step - loss: 7.8843 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9888 - age_output_loss: 1.4307 - weight_output_loss: 0.9878 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0424 - pose_output_loss: 0.9288 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5592 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4003 - weight_output_acc: 0.6359 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4418 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7153 - val_loss: 7.9408 - val_gender_output_loss: 0.6872 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4425 - val_weight_output_loss: 0.9981 - val_bag_output_loss: 0.9221 - val_footwear_output_loss: 1.0477 - val_pose_output_loss: 0.9307 - val_emotion_output_loss: 0.9368 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 17/50\n",
            "(0.3762499988079071, 0.401)\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.401.\n",
            "84/84 [==============================] - 31s 367ms/step - loss: 7.8843 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9888 - age_output_loss: 1.4307 - weight_output_loss: 0.9878 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0424 - pose_output_loss: 0.9288 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5592 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4003 - weight_output_acc: 0.6359 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4418 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7153 - val_loss: 7.9408 - val_gender_output_loss: 0.6872 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4425 - val_weight_output_loss: 0.9981 - val_bag_output_loss: 0.9221 - val_footwear_output_loss: 1.0477 - val_pose_output_loss: 0.9307 - val_emotion_output_loss: 0.9368 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 31s 371ms/step - loss: 7.8849 - gender_output_loss: 0.6898 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4295 - weight_output_loss: 0.9857 - bag_output_loss: 0.9197 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9256 - emotion_output_loss: 0.9050 - gender_output_acc: 0.5491 - image_quality_output_acc: 0.5495 - age_output_acc: 0.4014 - weight_output_acc: 0.6369 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4372 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7150 - val_loss: 7.9425 - val_gender_output_loss: 0.6918 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 1.0530 - val_pose_output_loss: 0.9384 - val_emotion_output_loss: 0.9392 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 18/50\n",
            "(0.4009999930858612, 0.42575)\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.42575.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8998 - gender_output_loss: 0.6944 - image_quality_output_loss: 0.9866 - age_output_loss: 1.4307 - weight_output_loss: 0.9854 - bag_output_loss: 0.9242 - footwear_output_loss: 1.0436 - pose_output_loss: 0.9337 - emotion_output_loss: 0.9012 - gender_output_acc: 0.5400 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4018 - weight_output_acc: 0.6367 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4410 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7158(0.4009999930858612, 0.42575)\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.9013 - gender_output_loss: 0.6946 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4305 - weight_output_loss: 0.9858 - bag_output_loss: 0.9237 - footwear_output_loss: 1.0438 - pose_output_loss: 0.9340 - emotion_output_loss: 0.9018 - gender_output_acc: 0.5396 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4018 - weight_output_acc: 0.6363 - bag_output_acc: 0.5642 - footwear_output_acc: 0.4399 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7155 - val_loss: 7.9618 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4389 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.9215 - val_footwear_output_loss: 1.0505 - val_pose_output_loss: 0.9542 - val_emotion_output_loss: 0.9390 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 19/50\n",
            "(0.4257499873638153, 0.45049999999999996)\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.45049999999999996.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8944 - gender_output_loss: 0.6896 - image_quality_output_loss: 0.9868 - age_output_loss: 1.4334 - weight_output_loss: 0.9859 - bag_output_loss: 0.9206 - footwear_output_loss: 1.0420 - pose_output_loss: 0.9321 - emotion_output_loss: 0.9039 - gender_output_acc: 0.5475 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4015 - weight_output_acc: 0.6359 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4395 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7146 - val_loss: 7.9629 - val_gender_output_loss: 0.7099 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4446 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0473 - val_pose_output_loss: 0.9424 - val_emotion_output_loss: 0.9373 - val_gender_output_acc: 0.4368 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 20/50\n",
            "(0.4505000114440918, 0.47525)\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.47525.\n",
            "84/84 [==============================] - 31s 375ms/step - loss: 7.8924 - gender_output_loss: 0.6925 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4294 - weight_output_loss: 0.9841 - bag_output_loss: 0.9193 - footwear_output_loss: 1.0452 - pose_output_loss: 0.9299 - emotion_output_loss: 0.9049 - gender_output_acc: 0.5475 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4010 - weight_output_acc: 0.6368 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4374 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7151 - val_loss: 7.9713 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9901 - val_age_output_loss: 1.4406 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9254 - val_footwear_output_loss: 1.0511 - val_pose_output_loss: 0.9582 - val_emotion_output_loss: 0.9353 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 21/50\n",
            "(0.4752500057220459, 0.5)\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.5.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8988 - gender_output_loss: 0.6895 - image_quality_output_loss: 0.9915 - age_output_loss: 1.4298 - weight_output_loss: 0.9890 - bag_output_loss: 0.9198 - footwear_output_loss: 1.0430 - pose_output_loss: 0.9327 - emotion_output_loss: 0.9035 - gender_output_acc: 0.5571 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4015 - weight_output_acc: 0.6348 - bag_output_acc: 0.5648 - footwear_output_acc: 0.4404 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7154(0.4752500057220459, 0.5)\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.5.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8993 - gender_output_loss: 0.6898 - image_quality_output_loss: 0.9921 - age_output_loss: 1.4306 - weight_output_loss: 0.9871 - bag_output_loss: 0.9194 - footwear_output_loss: 1.0431 - pose_output_loss: 0.9333 - emotion_output_loss: 0.9038 - gender_output_acc: 0.5554 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6361 - bag_output_acc: 0.5642 - footwear_output_acc: 0.4402 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7153 - val_loss: 7.9966 - val_gender_output_loss: 0.7025 - val_image_quality_output_loss: 0.9763 - val_age_output_loss: 1.4592 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.9293 - val_footwear_output_loss: 1.0614 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9417 - val_gender_output_acc: 0.4368 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 22/50\n",
            "(0.5, 0.47525)\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.47525.\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8929 - gender_output_loss: 0.6910 - image_quality_output_loss: 0.9892 - age_output_loss: 1.4305 - weight_output_loss: 0.9884 - bag_output_loss: 0.9202 - footwear_output_loss: 1.0433 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9031 - gender_output_acc: 0.5445 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4009 - weight_output_acc: 0.6372 - bag_output_acc: 0.5634 - footwear_output_acc: 0.4400 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7155 - val_loss: 7.9438 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9899 - val_age_output_loss: 1.4438 - val_weight_output_loss: 0.9879 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0478 - val_pose_output_loss: 0.9326 - val_emotion_output_loss: 0.9387 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 23/50\n",
            "(0.4752500057220459, 0.45049999999999996)\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.45049999999999996.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8983 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9919 - age_output_loss: 1.4298 - weight_output_loss: 0.9886 - bag_output_loss: 0.9220 - footwear_output_loss: 1.0434 - pose_output_loss: 0.9310 - emotion_output_loss: 0.9052 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4011 - weight_output_acc: 0.6362 - bag_output_acc: 0.5594 - footwear_output_acc: 0.4275 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7151(0.4752500057220459, 0.45049999999999996)\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8951 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9912 - age_output_loss: 1.4297 - weight_output_loss: 0.9884 - bag_output_loss: 0.9221 - footwear_output_loss: 1.0432 - pose_output_loss: 0.9307 - emotion_output_loss: 0.9033 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4009 - weight_output_acc: 0.6362 - bag_output_acc: 0.5592 - footwear_output_acc: 0.4280 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7160 - val_loss: 7.9443 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4414 - val_weight_output_loss: 0.9986 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.0484 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9401 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 24/50\n",
            "(0.4505000114440918, 0.42575)\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.42575.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.8906 - gender_output_loss: 0.6882 - image_quality_output_loss: 0.9872 - age_output_loss: 1.4327 - weight_output_loss: 0.9887 - bag_output_loss: 0.9229 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9010 - gender_output_acc: 0.5490 - image_quality_output_acc: 0.5509 - age_output_acc: 0.4002 - weight_output_acc: 0.6363 - bag_output_acc: 0.5574 - footwear_output_acc: 0.4437 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7154(0.4505000114440918, 0.42575)\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.42575.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8915 - gender_output_loss: 0.6882 - image_quality_output_loss: 0.9876 - age_output_loss: 1.4330 - weight_output_loss: 0.9885 - bag_output_loss: 0.9222 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9279 - emotion_output_loss: 0.9022 - gender_output_acc: 0.5490 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4001 - weight_output_acc: 0.6362 - bag_output_acc: 0.5580 - footwear_output_acc: 0.4432 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7149 - val_loss: 7.9354 - val_gender_output_loss: 0.6881 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0549 - val_pose_output_loss: 0.9321 - val_emotion_output_loss: 0.9366 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 25/50\n",
            "(0.4257499873638153, 0.401)\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.401.\n",
            "83/84 [============================>.] - ETA: 0s - loss: 7.9012 - gender_output_loss: 0.6919 - image_quality_output_loss: 0.9903 - age_output_loss: 1.4302 - weight_output_loss: 0.9865 - bag_output_loss: 0.9241 - footwear_output_loss: 1.0435 - pose_output_loss: 0.9284 - emotion_output_loss: 0.9062 - gender_output_acc: 0.5363 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4375 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7136Epoch 25/50\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8951 - gender_output_loss: 0.6917 - image_quality_output_loss: 0.9897 - age_output_loss: 1.4295 - weight_output_loss: 0.9843 - bag_output_loss: 0.9237 - footwear_output_loss: 1.0433 - pose_output_loss: 0.9284 - emotion_output_loss: 0.9044 - gender_output_acc: 0.5378 - image_quality_output_acc: 0.5507 - age_output_acc: 0.4008 - weight_output_acc: 0.6368 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4379 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7146 - val_loss: 7.9412 - val_gender_output_loss: 0.6864 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4406 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.9223 - val_footwear_output_loss: 1.0474 - val_pose_output_loss: 0.9323 - val_emotion_output_loss: 0.9449 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 26/50\n",
            "(0.4009999930858612, 0.37625)\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.37625.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8898 - gender_output_loss: 0.6897 - image_quality_output_loss: 0.9890 - age_output_loss: 1.4297 - weight_output_loss: 0.9845 - bag_output_loss: 0.9223 - footwear_output_loss: 1.0442 - pose_output_loss: 0.9279 - emotion_output_loss: 0.9025 - gender_output_acc: 0.5562 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4005 - weight_output_acc: 0.6362 - bag_output_acc: 0.5587 - footwear_output_acc: 0.4287 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7154 - val_loss: 7.9313 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.4402 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.9212 - val_footwear_output_loss: 1.0516 - val_pose_output_loss: 0.9307 - val_emotion_output_loss: 0.9371 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 27/50\n",
            "(0.3762499988079071, 0.3515)\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.3515.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8860 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9894 - age_output_loss: 1.4285 - weight_output_loss: 0.9866 - bag_output_loss: 0.9203 - footwear_output_loss: 1.0421 - pose_output_loss: 0.9263 - emotion_output_loss: 0.9041 - gender_output_acc: 0.5575 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4004 - weight_output_acc: 0.6357 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4297 - pose_output_acc: 0.6195 - emotion_output_acc: 0.7148 - val_loss: 7.9535 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4421 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9186 - val_footwear_output_loss: 1.0656 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9430 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.3728 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 28/50\n",
            "(0.351500004529953, 0.32675)\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.32675.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8893 - gender_output_loss: 0.6876 - image_quality_output_loss: 0.9878 - age_output_loss: 1.4298 - weight_output_loss: 0.9850 - bag_output_loss: 0.9232 - footwear_output_loss: 1.0450 - pose_output_loss: 0.9283 - emotion_output_loss: 0.9027 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4008 - weight_output_acc: 0.6357 - bag_output_acc: 0.5641 - footwear_output_acc: 0.4290 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7144 - val_loss: 7.9305 - val_gender_output_loss: 0.6900 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.4410 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 1.0511 - val_pose_output_loss: 0.9322 - val_emotion_output_loss: 0.9356 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 29/50\n",
            "(0.3267500102519989, 0.30200000000000005)\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.30200000000000005.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8834 - gender_output_loss: 0.6876 - image_quality_output_loss: 0.9878 - age_output_loss: 1.4295 - weight_output_loss: 0.9870 - bag_output_loss: 0.9200 - footwear_output_loss: 1.0437 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5582 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4011 - weight_output_acc: 0.6357 - bag_output_acc: 0.5632 - footwear_output_acc: 0.4356 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7153 - val_loss: 7.9500 - val_gender_output_loss: 0.6897 - val_image_quality_output_loss: 0.9769 - val_age_output_loss: 1.4535 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.9190 - val_footwear_output_loss: 1.0477 - val_pose_output_loss: 0.9371 - val_emotion_output_loss: 0.9364 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 30/50\n",
            "(0.3019999861717224, 0.27725)\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.27725.\n",
            "84/84 [==============================] - 31s 373ms/step - loss: 7.8874 - gender_output_loss: 0.6888 - image_quality_output_loss: 0.9870 - age_output_loss: 1.4301 - weight_output_loss: 0.9852 - bag_output_loss: 0.9224 - footwear_output_loss: 1.0466 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.5508 - age_output_acc: 0.3977 - weight_output_acc: 0.6360 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4214 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7149 - val_loss: 7.9352 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9769 - val_age_output_loss: 1.4405 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 0.9198 - val_footwear_output_loss: 1.0542 - val_pose_output_loss: 0.9329 - val_emotion_output_loss: 0.9375 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 31/50\n",
            "(0.2772499918937683, 0.2525)\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.2525.\n",
            "84/84 [==============================] - 31s 370ms/step - loss: 7.8760 - gender_output_loss: 0.6875 - image_quality_output_loss: 0.9856 - age_output_loss: 1.4277 - weight_output_loss: 0.9866 - bag_output_loss: 0.9205 - footwear_output_loss: 1.0415 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6360 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7152 - val_loss: 7.9355 - val_gender_output_loss: 0.6871 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4408 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9192 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9401 - val_emotion_output_loss: 0.9365 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 32/50\n",
            "(0.2524999976158142, 0.22774999999999998)\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.22774999999999998.\n",
            "84/84 [==============================] - 31s 370ms/step - loss: 7.8760 - gender_output_loss: 0.6875 - image_quality_output_loss: 0.9856 - age_output_loss: 1.4277 - weight_output_loss: 0.9866 - bag_output_loss: 0.9205 - footwear_output_loss: 1.0415 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6360 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7152 - val_loss: 7.9355 - val_gender_output_loss: 0.6871 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4408 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9192 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9401 - val_emotion_output_loss: 0.9365 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8756 - gender_output_loss: 0.6873 - image_quality_output_loss: 0.9857 - age_output_loss: 1.4305 - weight_output_loss: 0.9830 - bag_output_loss: 0.9191 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9269 - emotion_output_loss: 0.9024 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4003 - weight_output_acc: 0.6361 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4422 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7150 - val_loss: 7.9400 - val_gender_output_loss: 0.6863 - val_image_quality_output_loss: 0.9911 - val_age_output_loss: 1.4457 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0476 - val_pose_output_loss: 0.9312 - val_emotion_output_loss: 0.9361 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 33/50\n",
            "(0.2277500033378601, 0.20299999999999996)\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.20299999999999996.\n",
            "84/84 [==============================] - 32s 377ms/step - loss: 7.8680 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9864 - age_output_loss: 1.4272 - weight_output_loss: 0.9824 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0396 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5498 - age_output_acc: 0.4009 - weight_output_acc: 0.6365 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4430 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7149 - val_loss: 7.9241 - val_gender_output_loss: 0.6863 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9183 - val_footwear_output_loss: 1.0471 - val_pose_output_loss: 0.9321 - val_emotion_output_loss: 0.9363 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 34/50\n",
            "(0.2029999941587448, 0.17825000000000005)\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.17825000000000005.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8661 - gender_output_loss: 0.6879 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4279 - weight_output_loss: 0.9844 - bag_output_loss: 0.9166 - footwear_output_loss: 1.0402 - pose_output_loss: 0.9253 - emotion_output_loss: 0.8992 - gender_output_acc: 0.5590 - image_quality_output_acc: 0.5504 - age_output_acc: 0.4009 - weight_output_acc: 0.6363 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4467 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7154 - val_loss: 7.9254 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4382 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9184 - val_footwear_output_loss: 1.0489 - val_pose_output_loss: 0.9353 - val_emotion_output_loss: 0.9372 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 35/50\n",
            "(0.1782499998807907, 0.15350000000000003)\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.15350000000000003.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8700 - gender_output_loss: 0.6868 - image_quality_output_loss: 0.9851 - age_output_loss: 1.4269 - weight_output_loss: 0.9862 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0430 - pose_output_loss: 0.9261 - emotion_output_loss: 0.8977 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4009 - weight_output_acc: 0.6354 - bag_output_acc: 0.5630 - footwear_output_acc: 0.4323 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7163 - val_loss: 7.9205 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9782 - val_age_output_loss: 1.4367 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9199 - val_footwear_output_loss: 1.0478 - val_pose_output_loss: 0.9310 - val_emotion_output_loss: 0.9380 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 36/50\n",
            "(0.1535000056028366, 0.12875)\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.12875.\n",
            "84/84 [==============================] - 32s 375ms/step - loss: 7.8637 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9859 - age_output_loss: 1.4260 - weight_output_loss: 0.9826 - bag_output_loss: 0.9187 - footwear_output_loss: 1.0394 - pose_output_loss: 0.9253 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5634 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4009 - weight_output_acc: 0.6361 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7153 - val_loss: 7.9377 - val_gender_output_loss: 0.6876 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.4376 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9207 - val_footwear_output_loss: 1.0539 - val_pose_output_loss: 0.9309 - val_emotion_output_loss: 0.9377 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 37/50\n",
            "(0.1287499964237213, 0.10399999999999998)\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.10399999999999998.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8622 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9849 - age_output_loss: 1.4253 - weight_output_loss: 0.9819 - bag_output_loss: 0.9176 - footwear_output_loss: 1.0399 - pose_output_loss: 0.9263 - emotion_output_loss: 0.9001 - gender_output_acc: 0.5578 - image_quality_output_acc: 0.5493 - age_output_acc: 0.4010 - weight_output_acc: 0.6363 - bag_output_acc: 0.5640 - footwear_output_acc: 0.4469 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7149 - val_loss: 7.9265 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 1.0526 - val_pose_output_loss: 0.9312 - val_emotion_output_loss: 0.9371 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 38/50\n",
            "(0.10400000214576721, 0.07924999999999996)\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.07924999999999996.\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8622 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4261 - weight_output_loss: 0.9817 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0396 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9001 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4008 - weight_output_acc: 0.6362 - bag_output_acc: 0.5627 - footwear_output_acc: 0.4443 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7150 - val_loss: 7.9207 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4375 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9191 - val_footwear_output_loss: 1.0498 - val_pose_output_loss: 0.9312 - val_emotion_output_loss: 0.9385 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 39/50\n",
            "(0.07925000041723251, 0.05450000000000004)\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.05450000000000004.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8559 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9844 - age_output_loss: 1.4243 - weight_output_loss: 0.9816 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8979 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4015 - weight_output_acc: 0.6365 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7159 - val_loss: 7.9179 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9183 - val_footwear_output_loss: 1.0488 - val_pose_output_loss: 0.9308 - val_emotion_output_loss: 0.9352 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 40/50\n",
            "(0.054499998688697815, 0.029750000000000023)\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.029750000000000023.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8564 - gender_output_loss: 0.6865 - image_quality_output_loss: 0.9825 - age_output_loss: 1.4253 - weight_output_loss: 0.9816 - bag_output_loss: 0.9154 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9259 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4009 - weight_output_acc: 0.6364 - bag_output_acc: 0.5647 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7146 - val_loss: 7.9135 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9367 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 41/50\n",
            "(0.029750000685453415, 0.005)\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 378ms/step - loss: 7.8548 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4253 - weight_output_loss: 0.9825 - bag_output_loss: 0.9162 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9258 - emotion_output_loss: 0.8976 - gender_output_acc: 0.5634 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4012 - weight_output_acc: 0.6355 - bag_output_acc: 0.5642 - footwear_output_acc: 0.4466 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7159 - val_loss: 7.9127 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9763 - val_age_output_loss: 1.4364 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9363 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 42/50\n",
            "(0.004999999888241291, 0.029749999999999912)\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 382ms/step - loss: 7.8561 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4248 - weight_output_loss: 0.9818 - bag_output_loss: 0.9172 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9248 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5499 - age_output_acc: 0.4011 - weight_output_acc: 0.6363 - bag_output_acc: 0.5629 - footwear_output_acc: 0.4470 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7146 - val_loss: 7.9126 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0473 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9360 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 43/50\n",
            "(0.004999999888241291, 0.05450000000000004)\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 379ms/step - loss: 7.8557 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4251 - weight_output_loss: 0.9827 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0382 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8991 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5505 - age_output_acc: 0.4012 - weight_output_acc: 0.6357 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7150 - val_loss: 7.9127 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 44/50\n",
            "(0.004999999888241291, 0.07924999999999996)\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 376ms/step - loss: 7.8527 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9832 - age_output_loss: 1.4241 - weight_output_loss: 0.9818 - bag_output_loss: 0.9156 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8990 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4011 - weight_output_acc: 0.6360 - bag_output_acc: 0.5645 - footwear_output_acc: 0.4471 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7150 - val_loss: 7.9124 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0471 - val_pose_output_loss: 0.9304 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 45/50\n",
            "(0.004999999888241291, 0.10400000000000009)\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 384ms/step - loss: 7.8542 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9837 - age_output_loss: 1.4259 - weight_output_loss: 0.9818 - bag_output_loss: 0.9167 - footwear_output_loss: 1.0383 - pose_output_loss: 0.9246 - emotion_output_loss: 0.8979 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4001 - weight_output_acc: 0.6360 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7156 - val_loss: 7.9128 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 46/50\n",
            "(0.004999999888241291, 0.12875)\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 384ms/step - loss: 7.8542 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9837 - age_output_loss: 1.4259 - weight_output_loss: 0.9818 - bag_output_loss: 0.9167 - footwear_output_loss: 1.0383 - pose_output_loss: 0.9246 - emotion_output_loss: 0.8979 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4001 - weight_output_acc: 0.6360 - bag_output_acc: 0.5638 - footwear_output_acc: 0.4474 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7156 - val_loss: 7.9128 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "84/84 [==============================] - 32s 380ms/step - loss: 7.8547 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4257 - weight_output_loss: 0.9816 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5619 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4003 - weight_output_acc: 0.6360 - bag_output_acc: 0.5634 - footwear_output_acc: 0.4471 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7147 - val_loss: 7.9123 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0471 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9357 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 47/50\n",
            "(0.004999999888241291, 0.15349999999999991)\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8516 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9840 - age_output_loss: 1.4239 - weight_output_loss: 0.9815 - bag_output_loss: 0.9157 - footwear_output_loss: 1.0381 - pose_output_loss: 0.9249 - emotion_output_loss: 0.8983 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4013 - weight_output_acc: 0.6363 - bag_output_acc: 0.5642 - footwear_output_acc: 0.4476 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7154 - val_loss: 7.9125 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0471 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 48/50\n",
            "(0.004999999888241291, 0.17825000000000005)\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8560 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9837 - age_output_loss: 1.4256 - weight_output_loss: 0.9823 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9246 - emotion_output_loss: 0.8993 - gender_output_acc: 0.5619 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4006 - weight_output_acc: 0.6357 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4461 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7148 - val_loss: 7.9122 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 1.0471 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9356 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 49/50\n",
            "(0.004999999888241291, 0.20299999999999996)\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 382ms/step - loss: 7.8528 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9834 - age_output_loss: 1.4246 - weight_output_loss: 0.9817 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0377 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5502 - age_output_acc: 0.4008 - weight_output_acc: 0.6361 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4478 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7147 - val_loss: 7.9128 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9769 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0472 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n",
            "Epoch 50/50\n",
            "(0.004999999888241291, 0.2277500000000001)\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.005.\n",
            "84/84 [==============================] - 32s 381ms/step - loss: 7.8497 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9829 - age_output_loss: 1.4246 - weight_output_loss: 0.9801 - bag_output_loss: 0.9158 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8984 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4009 - weight_output_acc: 0.6372 - bag_output_acc: 0.5639 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7153 - val_loss: 7.9123 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9764 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5632 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3895 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.4353 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f375a6e1080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}